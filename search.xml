<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[build h2database source code under intellij idea]]></title>
      <url>https://selfpoised.github.io/2017/03/03/intellij%20build/</url>
      <content type="html"><![CDATA[<h3 id="pom-xml"><a href="#pom-xml" class="headerlink" title="pom.xml"></a>pom.xml</h3><p>h2代码并未直接使用mvn编译，而是使用了mvnw。并且其目标码为java1.6，而其mvn又依赖1.7。为了解决这个问题其使用了maven-toolchains-plugin：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">&lt;!-- Maven requires at least JRE 1.7 but we want to build with JDK 1.6 --&gt;</div><div class="line">      &lt;plugin&gt;</div><div class="line">        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</div><div class="line">        &lt;artifactId&gt;maven-toolchains-plugin&lt;/artifactId&gt;</div><div class="line">        &lt;version&gt;1.1&lt;/version&gt;</div><div class="line">        &lt;executions&gt;</div><div class="line">          &lt;execution&gt;</div><div class="line">            &lt;goals&gt;</div><div class="line">              &lt;goal&gt;toolchain&lt;/goal&gt;</div><div class="line">            &lt;/goals&gt;</div><div class="line">          &lt;/execution&gt;</div><div class="line">        &lt;/executions&gt;</div><div class="line">        &lt;configuration&gt;</div><div class="line">          &lt;toolchains&gt;</div><div class="line">            &lt;jdk&gt;</div><div class="line">              &lt;version&gt;1.6&lt;/version&gt;</div><div class="line">            &lt;/jdk&gt;</div><div class="line">          &lt;/toolchains&gt;</div><div class="line">        &lt;/configuration&gt;</div><div class="line">      &lt;/plugin&gt;</div></pre></td></tr></table></figure></p>
<p>此外，为了检查代码中没有使用jdk1.6之外的东西，又用了如下插件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">&lt;!-- Make sure we are not using anything outside JDK 1.6 --&gt;</div><div class="line">      &lt;plugin&gt;</div><div class="line">        &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;</div><div class="line">        &lt;artifactId&gt;animal-sniffer-maven-plugin&lt;/artifactId&gt;</div><div class="line">        &lt;version&gt;1.15&lt;/version&gt;</div><div class="line">        &lt;executions&gt;</div><div class="line">          &lt;execution&gt;</div><div class="line">            &lt;id&gt;check-java-api&lt;/id&gt;</div><div class="line">            &lt;phase&gt;test&lt;/phase&gt;</div><div class="line">            &lt;goals&gt;</div><div class="line">              &lt;goal&gt;check&lt;/goal&gt;</div><div class="line">            &lt;/goals&gt;</div><div class="line">          &lt;/execution&gt;</div><div class="line">        &lt;/executions&gt;</div><div class="line">        &lt;configuration&gt;</div><div class="line">          &lt;signature&gt;</div><div class="line">            &lt;groupId&gt;org.codehaus.mojo.signature&lt;/groupId&gt;</div><div class="line">            &lt;artifactId&gt;java16&lt;/artifactId&gt;</div><div class="line">            &lt;version&gt;1.1&lt;/version&gt;</div><div class="line">          &lt;/signature&gt;</div><div class="line">        &lt;/configuration&gt;</div><div class="line">      &lt;/plugin&gt;</div></pre></td></tr></table></figure></p>
<p>仅为学习和调试源码的情况下，可以将上述两个插件去掉，避免编译错误。</p>
<h3 id="idea-配置"><a href="#idea-配置" class="headerlink" title="idea 配置"></a>idea 配置</h3><p>1.添加依赖库<br><img src="http://ohz440knb.bkt.clouddn.com/h2-buildQQ%E5%9B%BE%E7%89%8720170303165203.png" alt="h2依赖库"></p>
<p>2.设置文件路径<br><img src="http://ohz440knb.bkt.clouddn.com/h2-buildQQ%E5%9B%BE%E7%89%8720170303165232.png" alt="路径设置"></p>
<p>特别注意：h2/src/tools要设置在”Test Source Folders”下面，否则会有文件找不到import</p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kafka offsets management in spark streaming]]></title>
      <url>https://selfpoised.github.io/2016/12/03/kafka%20offsets%20management/</url>
      <content type="html"><![CDATA[<p>kafka direct stream的出现，提高了与spark的适配能力与灵活性，可以精确实现一次性消费语义，<br>但是需要用户自行管理offsets。早期版本的offset是存储在zookeeper里面的，但据说对于大的消费场景来说，zookeeper的写性能就成了瓶颈，于是新的版本推荐把索引写入kafka内部队列：__consumer_offsets。</p>
<p>那么，怎么读写kafka offsets呢？如何指定zookeeper或者__consumer_offsets呢？<br>答案就在OffsetCommitRequest和OffsetFetchRequest里。</p>
<h4 id="OffsetFetchRequest-and-OffsetCommitRequest"><a href="#OffsetFetchRequest-and-OffsetCommitRequest" class="headerlink" title="OffsetFetchRequest and OffsetCommitRequest"></a>OffsetFetchRequest and OffsetCommitRequest</h4><p><strong>OffsetFetchRequest</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">case class OffsetFetchRequest(groupId: String,</div><div class="line">                              requestInfo: Seq[TopicAndPartition],</div><div class="line">                              versionId: Short = OffsetFetchRequest.CurrentVersion,</div><div class="line">                              correlationId: Int = 0,</div><div class="line">                              clientId: String = OffsetFetchRequest.DefaultClientId)</div></pre></td></tr></table></figure></p>
<p><strong>OffsetCommitRequest</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">case class OffsetCommitRequest(groupId: String,</div><div class="line">                requestInfo: immutable.Map[TopicAndPartition, OffsetAndMetadata],</div><div class="line">                versionId: Short = OffsetCommitRequest.CurrentVersion,</div><div class="line">                correlationId: Int = 0,</div><div class="line">                clientId: String = OffsetCommitRequest.DefaultClientId,</div><div class="line">                groupGenerationId: Int = org.apache.kafka.common.requests.OffsetCommitRequest.DEFAULT_GENERATION_ID,</div><div class="line">                memberId: String =  org.apache.kafka.common.requests.OffsetCommitRequest.DEFAULT_MEMBER_ID,</div><div class="line">                retentionMs: Long = org.apache.kafka.common.requests.OffsetCommitRequest.DEFAULT_RETENTION_TIME)</div></pre></td></tr></table></figure></p>
<p>versionId为0时指向zookeeper，大于0时则指向__consumer_offsets。</p>
<p>如何使用这两个类，可以参考：<a href="https://cwiki.apache.org/confluence/display/KAFKA/Committing+and+fetching+consumer+offsets+in+Kafka" target="_blank" rel="external">Committing and fetching consumer offsets in Kafka</a></p>
<hr>
<h4 id="KafkaCluster"><a href="#KafkaCluster" class="headerlink" title="KafkaCluster"></a>KafkaCluster</h4><p>在spark里面，如果直接使用上面两个类，显然不太方便，于是就有了KafkaCluster，是为了方便spark操作kafka cluster的封装。reference: <a href="https://github.com/apache/spark/blob/master/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/KafkaCluster.scala" target="_blank" rel="external">org/apache/spark/streaming/kafka/KafkaCluster.scala</a></p>
<p>提供broker.list等参数即可：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">class KafkaCluster(val kafkaParams: Map[String, String]) extends Serializable</div></pre></td></tr></table></figure></p>
<p>主要函数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def getConsumerOffsets(</div><div class="line">      groupId: String,</div><div class="line">      topicAndPartitions: Set[TopicAndPartition],</div><div class="line">      consumerApiVersion: Short</div><div class="line">    ): Either[Err, Map[TopicAndPartition, Long]]</div><div class="line"></div><div class="line">def setConsumerOffsets(</div><div class="line">      groupId: String,</div><div class="line">      offsets: Map[TopicAndPartition, Long],</div><div class="line">      consumerApiVersion: Short</div><div class="line">    ): Either[Err, Map[TopicAndPartition, Short]]</div><div class="line"></div><div class="line">def getPartitions(topics: Set[String]): Either[Err, Set[TopicAndPartition]]</div><div class="line"></div><div class="line">def getLatestLeaderOffsets(</div><div class="line">      topicAndPartitions: Set[TopicAndPartition]</div><div class="line">    ): Either[Err, Map[TopicAndPartition, LeaderOffset]]</div><div class="line">......</div><div class="line">......</div></pre></td></tr></table></figure></p>
<p>getConsumerOffsets与setConsumerOffsets便是offsets的读写函数，consumerApiVersion为0时，其存取媒介为zookeeper，大于0时为__consumer_offsets。</p>
<hr>
<h4 id="spark-streaming自动更新offsets"><a href="#spark-streaming自动更新offsets" class="headerlink" title="spark streaming自动更新offsets"></a>spark streaming自动更新offsets</h4><p>注册streamingContext监听事件：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">StreamingContext.addStreamingListener(new StreamingListener())</div></pre></td></tr></table></figure></p>
<p>在StreamingListener里不同事件位置实现offsets更新：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">trait StreamingListener &#123;</div><div class="line"></div><div class="line">  /** Called when a receiver has been started */</div><div class="line">  def onReceiverStarted(receiverStarted: StreamingListenerReceiverStarted) &#123; &#125;</div><div class="line"></div><div class="line">  /** Called when a receiver has reported an error */</div><div class="line">  def onReceiverError(receiverError: StreamingListenerReceiverError) &#123; &#125;</div><div class="line"></div><div class="line">  /** Called when a receiver has been stopped */</div><div class="line">  def onReceiverStopped(receiverStopped: StreamingListenerReceiverStopped) &#123; &#125;</div><div class="line"></div><div class="line">  /** Called when a batch of jobs has been submitted for processing. */</div><div class="line">  def onBatchSubmitted(batchSubmitted: StreamingListenerBatchSubmitted) &#123; &#125;</div><div class="line"></div><div class="line">  /** Called when processing of a batch of jobs has started.  */</div><div class="line">  def onBatchStarted(batchStarted: StreamingListenerBatchStarted) &#123; &#125;</div><div class="line"></div><div class="line">  /** Called when processing of a batch of jobs has completed. */</div><div class="line">  def onBatchCompleted(batchCompleted: StreamingListenerBatchCompleted) &#123; &#125;</div><div class="line"></div><div class="line">  /** Called when processing of a job of a batch has started. */</div><div class="line">  def onOutputOperationStarted(</div><div class="line">      outputOperationStarted: StreamingListenerOutputOperationStarted) &#123; &#125;</div><div class="line"></div><div class="line">  /** Called when processing of a job of a batch has completed. */</div><div class="line">  def onOutputOperationCompleted(</div><div class="line">      outputOperationCompleted: StreamingListenerOutputOperationCompleted) &#123; &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<p><img src="http://ohz440knb.bkt.clouddn.com/kafka%20schema15338474_677916559048913_6357551582430625792_n.jpg" alt="lebanon tastes"></p>
<hr>
<h4 id="一个基于KafkaCluster的完整offsets操作实现"><a href="#一个基于KafkaCluster的完整offsets操作实现" class="headerlink" title="一个基于KafkaCluster的完整offsets操作实现"></a>一个基于KafkaCluster的完整offsets操作实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div></pre></td><td class="code"><pre><div class="line">import kafka.common.TopicAndPartition</div><div class="line">import kafka.message.MessageAndMetadata</div><div class="line">import kafka.serializer.Decoder</div><div class="line">import org.apache.log4j.Logger</div><div class="line">import org.apache.spark.SparkException</div><div class="line">import org.apache.spark.rdd.RDD</div><div class="line">import org.apache.spark.streaming.StreamingContext</div><div class="line">import org.apache.spark.streaming.dstream.InputDStream</div><div class="line">import org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset</div><div class="line">import org.apache.spark.streaming.kafka.&#123;HasOffsetRanges, KafkaCluster, KafkaUtils&#125;</div><div class="line"></div><div class="line">import scala.reflect.ClassTag</div><div class="line"></div><div class="line">/**</div><div class="line">  * KafkaCluster encapsulation, provides mainly offset management methods</div><div class="line">  *</div><div class="line">  * offsetSavingMode == KafkaEnums.zookeeper means offset saved in zk</div><div class="line">  * offsetSavingMode == KafkaEnums.kafka means offset saved in kafka</div><div class="line">  */</div><div class="line">class KafkaManager(val kafkaParams: Map[String, String], val offsetSavingMode: KafkaEnums) &#123;</div><div class="line">  private val kc = new KafkaCluster(kafkaParams)</div><div class="line"></div><div class="line">  /**</div><div class="line">    * consumerApiVersion == 0 means offset saved in zk</div><div class="line">    * consumerApiVersion == 1 means offset saved in kafka</div><div class="line">    */</div><div class="line">  def consumerApiVersion = if(offsetSavingMode == KafkaEnums.kafka) &#123;1.toShort&#125; else &#123;0.toShort&#125;</div><div class="line"></div><div class="line">  /**</div><div class="line">    * 由于kafka本身的消息留存机制，用户保存的offset可能与队列实际</div><div class="line">    * 情况不一致。该函数对用户保存offset进行合理重置</div><div class="line">    *</div><div class="line">    */</div><div class="line">  private def setOrUpdateOffsets(topics: Set[String], groupId: String): Unit = &#123;</div><div class="line">    topics.foreach(topic =&gt; &#123;</div><div class="line">      val partitionsE = kc.getPartitions(Set(topic))</div><div class="line">      if (partitionsE.isLeft)&#123;</div><div class="line">        val err = s&quot;get kafka partition failed: $&#123;partitionsE.left.get&#125; for topics &quot; + topics</div><div class="line">        logger.error(err)</div><div class="line">        throw new SparkException(err)</div><div class="line">      &#125;</div><div class="line"></div><div class="line">      val partitions = partitionsE.right.get</div><div class="line">      val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions, consumerApiVersion)</div><div class="line">      // 若某个groupid首次消费，则没有offset信息，会报错，从头开始读</div><div class="line">      logger.warn(&quot;consumerApiVersion: &quot; + consumerApiVersion)</div><div class="line">      // offset存储媒介为zk，此处isLeft为true，表示未曾消费过</div><div class="line">      // 存储媒介为kafka时，此处是有右值的，但是其offset为-1，小于earliestLeaderOffset，</div><div class="line">      // 因而会被重置为0，即从头开始消费</div><div class="line">      if (consumerOffsetsE.isLeft)&#123; // 未曾消费过</div><div class="line">        val reset = kafkaParams.get(&quot;auto.offset.reset&quot;).map(_.toLowerCase)</div><div class="line">        var leaderOffsets: Map[TopicAndPartition, LeaderOffset] = null</div><div class="line"></div><div class="line">        if (reset == Some(&quot;smallest&quot;)) &#123;// 从头消费</div><div class="line">          val leaderOffsetsE = kc.getEarliestLeaderOffsets(partitions)</div><div class="line">          if (leaderOffsetsE.isLeft)&#123;</div><div class="line">            val err = s&quot;get earliest leader offsets failed: $&#123;leaderOffsetsE.left.get&#125; for partitions &quot; + partitions</div><div class="line">            logger.error(err)</div><div class="line">            throw new SparkException(err)</div><div class="line">          &#125;</div><div class="line">          leaderOffsets = leaderOffsetsE.right.get</div><div class="line">        &#125; else &#123; // 从最新offset处消费</div><div class="line">          val leaderOffsetsE = kc.getLatestLeaderOffsets(partitions)</div><div class="line">          if (leaderOffsetsE.isLeft)&#123;</div><div class="line">            val err = s&quot;get latest leader offsets failed: $&#123;leaderOffsetsE.left.get&#125; for partitions &quot; + partitions</div><div class="line">            logger.error(err)</div><div class="line">            throw new SparkException(err)</div><div class="line">          &#125;</div><div class="line">          leaderOffsets = leaderOffsetsE.right.get</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        val offsets = leaderOffsets.map &#123;case (tp, offset) =&gt; (tp, offset.offset)&#125;</div><div class="line">        kc.setConsumerOffsets(groupId, offsets, consumerApiVersion)</div><div class="line">      &#125;else&#123; // 消费过</div><div class="line">        /**</div><div class="line">          * 如果streaming程序执行的时候出现kafka.common.OffsetOutOfRangeException，</div><div class="line">          * 说明保存的offsets已经过时了，即kafka的定时清理策略已经将包含该offsets的文件删除。</div><div class="line">          * 针对这种情况，只要判断一下保存的consumerOffsets和earliestLeaderOffsets的大小，</div><div class="line">          * 如果consumerOffsets比earliestLeaderOffsets还小的话，说明consumerOffsets已过时,</div><div class="line">          * 这时把consumerOffsets更新为earliestLeaderOffsets</div><div class="line">          */</div><div class="line">        val earliestLeaderOffsetsE = kc.getEarliestLeaderOffsets(partitions)</div><div class="line">        if (earliestLeaderOffsetsE.isLeft)&#123;</div><div class="line">          val err = s&quot;get earliest leader offsets failed: $&#123;earliestLeaderOffsetsE.left.get&#125; for partitions &quot; + partitions</div><div class="line">          logger.error(err)</div><div class="line">          throw new SparkException(err)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        val earliestLeaderOffsets = earliestLeaderOffsetsE.right.get</div><div class="line">        val consumerOffsets = consumerOffsetsE.right.get</div><div class="line">        // 可能只是部分分区consumerOffsets过时，</div><div class="line">        // 所以只更新过时分区的consumerOffsets为earliestLeaderOffsets</div><div class="line">        var offsets: Map[TopicAndPartition, Long] = Map()</div><div class="line">        consumerOffsets.foreach(&#123;</div><div class="line">          case(tp, cOffset) =&gt;</div><div class="line">          &#123;</div><div class="line">            val earliestLeaderOffset = earliestLeaderOffsets(tp).offset</div><div class="line">            if (cOffset &lt; earliestLeaderOffset) &#123;</div><div class="line">              logger.warn(&quot;consumer group:&quot; + groupId + &quot;,topic:&quot; + tp.topic +</div><div class="line">                &quot;,partition:&quot; + tp.partition + &quot; offsets已经过时，更新为&quot; + earliestLeaderOffset)</div><div class="line"></div><div class="line">              offsets += (tp -&gt; earliestLeaderOffset)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;)</div><div class="line"></div><div class="line">        if (!offsets.isEmpty) &#123;</div><div class="line">          kc.setConsumerOffsets(groupId, offsets, consumerApiVersion)</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def updateOffsets(rdd: RDD[(Object, Object, String)]) : Unit = &#123;</div><div class="line">    val groupId = kafkaParams.get(&quot;group.id&quot;).get</div><div class="line">    val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges</div><div class="line">    for (offsets &lt;- offsetsList) &#123;</div><div class="line">      val topicAndPartition = TopicAndPartition(offsets.topic, offsets.partition)</div><div class="line">      val o = kc.setConsumerOffsets(groupId, Map((topicAndPartition, offsets.untilOffset)), consumerApiVersion)</div><div class="line">      if (o.isLeft) &#123;</div><div class="line">        logger.error(s&quot;Error updating the offset to Kafka cluster: $&#123;o.left.get&#125; for group &quot; + groupId)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def updateOffsets(offset: Map[TopicAndPartition, Long]) : Unit = &#123;</div><div class="line">    val groupId = kafkaParams.get(&quot;group.id&quot;).get</div><div class="line">    val o = kc.setConsumerOffsets(groupId, offset, consumerApiVersion)</div><div class="line">    if (o.isLeft) &#123;</div><div class="line">      logger.error(s&quot;Error updating the offset to Kafka cluster: $&#123;o.left.get&#125; for group &quot; + groupId + &quot; &quot; + offset)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def restoreOffsets(topics: Set[String]): Map[TopicAndPartition, Long] =&#123;</div><div class="line">    val groupId = kafkaParams.get(&quot;group.id&quot;).get</div><div class="line"></div><div class="line">    // 读取offsets前先根据实际情况更新offsets</div><div class="line">    setOrUpdateOffsets(topics, groupId)</div><div class="line"></div><div class="line">    val partitionsE = kc.getPartitions(topics)</div><div class="line">    if (partitionsE.isLeft)&#123;</div><div class="line">      val err = s&quot;get kafka partition failed: $&#123;partitionsE.left.get&#125; for topics &quot; + topics</div><div class="line">      logger.error(err)</div><div class="line">      throw new SparkException(err)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    val partitions = partitionsE.right.get</div><div class="line">    val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions, consumerApiVersion)</div><div class="line">    if (consumerOffsetsE.isLeft)&#123;</div><div class="line">      val err = s&quot;get kafka consumer offsets failed: $&#123;consumerOffsetsE.left.get&#125; for group &quot; + groupId</div><div class="line">      logger.error(err)</div><div class="line">      throw new SparkException(err)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    consumerOffsetsE.right.get</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[kafka schema of the Confluent platform]]></title>
      <url>https://selfpoised.github.io/2016/12/03/kafka%20schema/</url>
      <content type="html"><![CDATA[<p>在消费kafka消息的时候，可能会有疑问，消息里是否自带schema，schema是如何在反序列化<br>消息的时候起作用的。实际上在conflument平台上，kafka消息格式如下：</p>
<table>
<thead>
<tr>
<th>Bytes</th>
<th>Area</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>Magic Byte</td>
<td>Confluent serialization format version number; currently always 0.</td>
</tr>
<tr>
<td>1-4</td>
<td>Schema ID</td>
<td>4-byte schema ID as returned by the Schema Registry</td>
</tr>
<tr>
<td>5</td>
<td>Data</td>
<td>Avro serialized data in Avro’s binary encoding. The only exception is raw bytes, which will be written directly without any special Avro encoding.</td>
</tr>
</tbody>
</table>
<hr>
<h4 id="生产端"><a href="#生产端" class="headerlink" title="生产端"></a>生产端</h4><p>encoder获取schema并且向schema registry请求一个schema id，若已存在则直接返回，若没有则新产生一个并注册<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">import org.apache.avro.Schema;</div><div class="line">import org.apache.avro.generic.GenericData;</div><div class="line">import org.apache.avro.generic.GenericRecord;</div><div class="line">import org.apache.kafka.clients.producer.KafkaProducer;</div><div class="line">import org.apache.kafka.clients.producer.ProducerConfig;</div><div class="line">import org.apache.kafka.clients.producer.ProducerRecord;</div><div class="line">import java.util.Properties;</div><div class="line"></div><div class="line">Properties props = new Properties();</div><div class="line">props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;);</div><div class="line">props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,</div><div class="line">          io.confluent.kafka.serializers.KafkaAvroSerializer.class);</div><div class="line">props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,</div><div class="line">          io.confluent.kafka.serializers.KafkaAvroSerializer.class);</div><div class="line">props.put(&quot;schema.registry.url&quot;, &quot;http://localhost:8081&quot;);</div><div class="line">KafkaProducer producer = new KafkaProducer(props);</div><div class="line"></div><div class="line">String key = &quot;key1&quot;;</div><div class="line">String userSchema = &quot;&#123;\&quot;type\&quot;:\&quot;record\&quot;,&quot; +</div><div class="line">                    &quot;\&quot;name\&quot;:\&quot;myrecord\&quot;,&quot; +</div><div class="line">                    &quot;\&quot;fields\&quot;:[&#123;\&quot;name\&quot;:\&quot;f1\&quot;,\&quot;type\&quot;:\&quot;string\&quot;&#125;]&#125;&quot;;</div><div class="line">Schema.Parser parser = new Schema.Parser();</div><div class="line">Schema schema = parser.parse(userSchema);</div><div class="line">GenericRecord avroRecord = new GenericData.Record(schema);</div><div class="line">avroRecord.put(&quot;f1&quot;, &quot;value1&quot;);</div><div class="line"></div><div class="line">ProducerRecord&lt;Object, Object&gt; record = new ProducerRecord&lt;&gt;(&quot;topic1&quot;, key, avroRecord);</div><div class="line">try &#123;</div><div class="line">  producer.send(record);</div><div class="line">&#125; catch(SerializationException e) &#123;</div><div class="line">  // may need to do something with it</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<p><img src="http://ohz440knb.bkt.clouddn.com/kafka%20schema14547664_340090316367092_266735473584504832_n.jpg" alt="indian peacock"></p>
<hr>
<h4 id="消费端"><a href="#消费端" class="headerlink" title="消费端"></a>消费端</h4><p>根据shema id从schema registry请求schema，并且在本地缓存<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">import org.apache.avro.generic.IndexedRecord;</div><div class="line">import kafka.consumer.ConsumerConfig;</div><div class="line">import kafka.consumer.ConsumerIterator;</div><div class="line">import kafka.consumer.KafkaStream;</div><div class="line">import kafka.javaapi.consumer.ConsumerConnector;</div><div class="line">import io.confluent.kafka.serializers.KafkaAvroDecoder;</div><div class="line">import kafka.message.MessageAndMetadata;</div><div class="line">import kafka.utils.VerifiableProperties;</div><div class="line">import org.apache.kafka.common.errors.SerializationException;</div><div class="line">import java.util.*;</div><div class="line"></div><div class="line">Properties props = new Properties();</div><div class="line">props.put(&quot;zookeeper.connect&quot;, &quot;localhost:2181&quot;);</div><div class="line">props.put(&quot;group.id&quot;, &quot;group1&quot;);</div><div class="line">props.put(&quot;schema.registry.url&quot;, &quot;http://localhost:8081&quot;);</div><div class="line"></div><div class="line">String topic = &quot;topic1&quot;;</div><div class="line">Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;&gt;();</div><div class="line">topicCountMap.put(topic, new Integer(1));</div><div class="line"></div><div class="line">VerifiableProperties vProps = new VerifiableProperties(props);</div><div class="line">KafkaAvroDecoder keyDecoder = new KafkaAvroDecoder(vProps);</div><div class="line">KafkaAvroDecoder valueDecoder = new KafkaAvroDecoder(vProps);</div><div class="line"></div><div class="line">ConsumerConnector consumer = kafka.consumer.Consumer.createJavaConsumerConnector(new ConsumerConfig(props));</div><div class="line"></div><div class="line">Map&lt;String, List&lt;KafkaStream&lt;Object, Object&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(</div><div class="line">    topicCountMap, keyDecoder, valueDecoder);</div><div class="line">KafkaStream stream = consumerMap.get(topic).get(0);</div><div class="line">ConsumerIterator it = stream.iterator();</div><div class="line">while (it.hasNext()) &#123;</div><div class="line">  MessageAndMetadata messageAndMetadata = it.next();</div><div class="line">  try &#123;</div><div class="line">    String key = (String) messageAndMetadata.key();</div><div class="line">    IndexedRecord value = (IndexedRecord) messageAndMetadata.message();</div><div class="line"></div><div class="line">    ...</div><div class="line">  &#125; catch(SerializationException e) &#123;</div><div class="line">    // may need to do something with it</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><p><a href="http://stackoverflow.com/questions/31204201/apache-kafka-with-avro-and-schema-repo-where-in-the-message-does-the-schema-id" target="_blank" rel="external">Apache Kafka with Avro and Schema Repo - where in the message does the schema Id go?</a><br><a href="http://docs.confluent.io/3.1.1/schema-registry/docs/serializer-formatter.html" target="_blank" rel="external">Serializer and Formatter</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Hello Strangers]]></title>
      <url>https://selfpoised.github.io/2016/12/03/hello-strangers/</url>
      <content type="html"><![CDATA[<p>Welcome to my personal blog, more to come later!</p>
]]></content>
    </entry>
    
  
  
</search>
